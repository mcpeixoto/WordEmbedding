{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data.txt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read data.txt line by line\n",
    "with open('data.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataclass from torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a class for the dataset\n",
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, lines, context_size=2):\n",
    "        # Transform args to self\n",
    "        for key, value in locals().items():\n",
    "            if key != 'self':\n",
    "                setattr(self, key, value)\n",
    "                \n",
    "        # Read data.txt line by line\n",
    "        with open('data.txt', 'r') as f:\n",
    "            self.lines = f.readlines()\n",
    "\n",
    "        # Process lines\n",
    "        self.process_lines()\n",
    "\n",
    "        # Concatenate all lines\n",
    "        self.text = ' '.join(self.lines)\n",
    "\n",
    "        # Create a list of words\n",
    "        self.words = self.text.split()\n",
    "        self.vocab_size = len(set(self.words))\n",
    "\n",
    "\n",
    "        # Create a dictionary of words\n",
    "        # for one-hot encoding\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(set(self.words))}\n",
    "        self.idx2word = {idx: word for idx, word in enumerate(set(self.words))}\n",
    "\n",
    "        # Convert words to vectors with one-hot encoding\n",
    "        self.words = [self.one_hot_encode(word) for word in tqdm(self.words, desc='One-hot encoding')]\n",
    "        \n",
    "        data = []\n",
    "        target = []\n",
    "        # Create a list of tuples\n",
    "        # (next_word, [context_words])\n",
    "        for i in tqdm(range(context_size, len(self.words) - context_size), desc='Preparing data'):\n",
    "            context = []\n",
    "            for j in range(i - context_size, i):\n",
    "                context.append(self.words[j])\n",
    "            data.append(context)\n",
    "            target.append(self.words[i])\n",
    "\n",
    "        # Convert to numpy array, this makes it faster aparently\n",
    "        data = np.array(data)\n",
    "        target = np.array(target)\n",
    "\n",
    "        # Here data has the shape of (n_samples, context_size, vocab_size)\n",
    "        # Target has the shape of (n_samples, vocab_size)\n",
    "        # Let's reshape data to (n_samples, context_size * vocab_size)\n",
    "        data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "            \n",
    "        # Convert to torch tensor\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    def process_lines(self):\n",
    "        self.lines = [line.lower() for line in self.lines]\n",
    "        self.lines = [line.replace('\\n', '') for line in self.lines]\n",
    "        self.lines = [''.join([c for c in line if c.isalnum() or c == ' ']) for line in self.lines]\n",
    "            \n",
    "    # One-hot encoding\n",
    "    def one_hot_encode(self, word):\n",
    "        x = np.zeros(len(self.word2idx))\n",
    "        x[self.word2idx[word]] = 1\n",
    "        return x\n",
    "\n",
    "    def one_hot_decode(self, x):\n",
    "        return self.idx2word[np.argmax(x)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.data, self.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One-hot encoding: 100%|██████████| 24042/24042 [00:00<00:00, 67706.98it/s]\n",
      "Preparing data: 100%|██████████| 24032/24032 [00:00<00:00, 681732.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the dataset\n",
    "dataset = WordEmbeddingDataset(lines, context_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 241/241 [00:02<00:00, 87.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.00027107\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 241/241 [00:02<00:00, 113.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.00027485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 241/241 [00:02<00:00, 113.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.00027394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 241/241 [00:02<00:00, 113.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.00026483\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 241/241 [00:02<00:00, 113.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.00026713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 241/241 [00:02<00:00, 113.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.00026533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 241/241 [00:02<00:00, 113.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.00025007\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 241/241 [00:02<00:00, 113.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.00024246\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 241/241 [00:02<00:00, 113.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.00023827\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 241/241 [00:02<00:00, 113.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.00022381\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 241/241 [00:02<00:00, 113.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.00022083\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 241/241 [00:02<00:00, 113.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.00022672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 241/241 [00:02<00:00, 113.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.00022082\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 241/241 [00:02<00:00, 113.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.00022307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 241/241 [00:02<00:00, 113.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.00022091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 241/241 [00:02<00:00, 113.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.00022189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 241/241 [00:02<00:00, 113.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 0.00021555\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 241/241 [00:02<00:00, 113.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Loss: 0.00021121\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 241/241 [00:02<00:00, 113.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Loss: 0.00021241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 241/241 [00:02<00:00, 113.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 0.00020656\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 241/241 [00:02<00:00, 113.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss: 0.00020472\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 241/241 [00:02<00:00, 113.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Loss: 0.00020794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 241/241 [00:02<00:00, 113.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Loss: 0.00020514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 241/241 [00:02<00:00, 113.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Loss: 0.00020553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 241/241 [00:02<00:00, 113.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss: 0.00020261\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 241/241 [00:02<00:00, 113.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Loss: 0.00020574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 241/241 [00:02<00:00, 113.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Loss: 0.00020217\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 241/241 [00:02<00:00, 113.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Loss: 0.00020226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 241/241 [00:02<00:00, 113.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Loss: 0.00020184\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 241/241 [00:02<00:00, 113.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss: 0.00020135\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 241/241 [00:02<00:00, 113.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Loss: 0.00020248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 241/241 [00:02<00:00, 113.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Loss: 0.00019887\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 241/241 [00:02<00:00, 113.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Loss: 0.00020113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 241/241 [00:02<00:00, 113.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Loss: 0.00019870\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 241/241 [00:02<00:00, 113.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Loss: 0.00021201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 241/241 [00:02<00:00, 113.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Loss: 0.00022449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 241/241 [00:02<00:00, 113.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Loss: 0.00020745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 241/241 [00:02<00:00, 113.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Loss: 0.00020399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 241/241 [00:02<00:00, 112.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Loss: 0.00020416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 241/241 [00:02<00:00, 112.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Loss: 0.00020468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 241/241 [00:02<00:00, 112.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Loss: 0.00020406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 241/241 [00:02<00:00, 112.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Loss: 0.00021894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 241/241 [00:02<00:00, 112.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Loss: 0.00021604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 241/241 [00:15<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Loss: 0.00022354\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "# Inicialize our model\n",
    "from torch import nn\n",
    "\n",
    "# Create a class for the model\n",
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        \n",
    "        # This will be a NN with 3 layers\n",
    "        # 1. Input layer\n",
    "        # 2. Hidden layer\n",
    "        # 3. Output layer\n",
    "\n",
    "        # Input layer\n",
    "        self.lay1 = nn.Linear(vocab_size * context_size, 128)\n",
    "        # Activation function\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.lay2 = nn.Linear(128, embedding_dim)\n",
    "        # Activation function\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.lay3 = nn.Linear(embedding_dim, vocab_size)\n",
    "        # Activation function\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Output layer\n",
    "        self.lay4 = nn.Linear(vocab_size, vocab_size)\n",
    "        # Activation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lay1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.lay2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.lay3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.lay4(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def embedd(self, x):\n",
    "        x = self.lay1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.lay2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "# which will try to predict the next word\n",
    "# given a word\n",
    "model = WordEmbeddingModel(vocab_size=dataset.vocab_size, embedding_dim=20, context_size=5).to(device)\n",
    "\n",
    "# Create a loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create a data loader\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "max_patience = 10\n",
    "patience = 0\n",
    "\n",
    "previous_loss = np.inf\n",
    "for epoch in range(100):\n",
    "    for i, batch in tqdm(enumerate(dataloader), desc=f'Epoch {epoch + 1}', total=len(dataloader), leave=True):\n",
    "        data, target = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(data)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, target)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss.item():.8f}')\n",
    "\n",
    "    # Save\n",
    "    if loss.item() < previous_loss:\n",
    "        previous_loss = loss.item()\n",
    "        print('Saving model...')\n",
    "        torch.save(model.state_dict(), f'model.pth')\n",
    "        patience = 0\n",
    "\n",
    "    # Early stopping\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience == max_patience:\n",
    "            print('Early stopping...')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the results of the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words\n",
    "words = dataset.words\n",
    "words = [dataset.one_hot_decode(word) for word in words]\n",
    "\n",
    "# Predict next word\n",
    "def predict_next_word(context):\n",
    "    # Convert to one-hot\n",
    "    context = [dataset.one_hot_encode(word) for word in context]\n",
    "    # Flatten\n",
    "    context = [item for sublist in context for item in sublist]\n",
    "    # Convert to tensor\n",
    "    context = torch.tensor(context, dtype=torch.float32).to(device)\n",
    "    # Reshape\n",
    "    context = context.reshape(1, context.shape[0])\n",
    "    # Predict\n",
    "    y_pred = model(context)\n",
    "    # Get the word\n",
    "    word = words[np.argmax(y_pred.detach().cpu().numpy())]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blob'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word(['inform', 'the', 'commander', 'that', 'lord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coughing'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word(['the', 'commander', 'that', 'lord', 'blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word(['commander', 'that', 'lord', 'blob', 'coughing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1742, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1596, 0.0000, 0.0000,\n",
       "         0.0000, 3.2487]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedd a sentence\n",
    "def embedd_sentence(sentence):\n",
    "    # Convert to one-hot\n",
    "    sentence = [dataset.one_hot_encode(word) for word in sentence]\n",
    "    # Flatten\n",
    "    sentence = [item for sublist in sentence for item in sublist]\n",
    "    # Convert to tensor\n",
    "    sentence = torch.tensor(sentence, dtype=torch.float32).to(device)\n",
    "    # Reshape\n",
    "    sentence = sentence.reshape(1, sentence.shape[0])\n",
    "    # Embedd\n",
    "    embedding = model.embedd(sentence)\n",
    "    return embedding\n",
    "\n",
    "embedd_sentence(['inform', 'the', 'commander', 'that', 'lord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7af48fcad31774be64aab1730474abe33b1476e52bb650fdbd242e7416b97765"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
